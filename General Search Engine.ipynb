{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from scipy import spatial\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import collections\n",
    "import heapq\n",
    "from IPython.display import HTML\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "import webbrowser\n",
    "import datetime\n",
    "from langdetect import detect\n",
    "import io\n",
    "from multiprocessing import Pool\n",
    "import multi_processing_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2. Useful Functions (read, write, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(filename, content):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, \"w\", encoding='utf-8' ) as f:\n",
    "        f.write(str(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tsv(filename, content):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, \"wt\", newline='', encoding='utf-8' ) as out_file:\n",
    "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "        tsv_writer.writerow(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(filename, type_='utf-8'):\n",
    "    # cp850\n",
    "    with open(filename, encoding = type_) as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for data in reader:\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(file_name, content):\n",
    "    os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        json.dump(content, outfile, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonKeys2int(x):\n",
    "    if isinstance(x, dict):\n",
    "            return {int(k):v for k,v in x.items()}\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_name):\n",
    "    with open(file_name) as json_file:\n",
    "        data_dict = json.load(json_file, object_hook=jsonKeys2int)\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tsv_files(path, destiny='data/tsv_files/tsv_files.tsv'):\n",
    "    tsv_files = os.listdir(path)\n",
    "    list_tsv = []\n",
    "    i = 0\n",
    "    for filename in tsv_files:\n",
    "        i += 1\n",
    "        if i%1000 == 0:\n",
    "            print(i)\n",
    "        book_id = re.findall(r'\\d+', filename)[0]\n",
    "        filename = path + filename\n",
    "        book_data = read_tsv(filename)\n",
    "        book_data = [book_id] + book_data\n",
    "        list_tsv.append(book_data)\n",
    "    df=pd.DataFrame(list_tsv,columns=['bookid', 'bookTitle', 'bookSeries', 'bookAuthors', 'ratingValue', \n",
    "                                     'ratingCount', 'reviewCount', 'Plot', 'PublishingDate', \n",
    "                                     'characters'])\n",
    "    df.to_csv(destiny, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_book(href):\n",
    "    driver.get(href)       \n",
    "    time.sleep(5)\n",
    "    return driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/book_urls.txt'\n",
    "\n",
    "book_urls = open(filename, 'r')\n",
    "count = 1\n",
    "for url in book_urls:\n",
    "    if count > 0:\n",
    "        print(url)\n",
    "        page_number = int((count-1)/100)+1\n",
    "        html = scrap_book(url)\n",
    "        path = 'data/page_'+str(page_number)+'/article_'+str(count)+'.html'\n",
    "        print(path)\n",
    "        write_file(path, html)\n",
    "    count +=1\n",
    "    if count == 2001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_to_number = {'January': 1,\n",
    "                   'February': 2,\n",
    "                   'March': 3,\n",
    "                   'April': 4,\n",
    "                   'May': 5,\n",
    "                   'June': 6,\n",
    "                   'July': 7,\n",
    "                   'August': 8,\n",
    "                   'September': 9,\n",
    "                   'October': 10,\n",
    "                   'November': 11,\n",
    "                   'December': 12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html_in_folder(path):\n",
    "    for html_file in os.listdir(path):\n",
    "        print(html_file)\n",
    "        with open(path + '/' + html_file, encoding='utf8') as infile:\n",
    "            soup = BeautifulSoup(infile, features=\"lxml\")\n",
    "            # Plot can be hidden (if it is hidden we have to take the complete plot)\n",
    "            try:\n",
    "                Plot = ' '.join([remove_html_tags(str(c)) for c in soup.find_all('div', id=\"description\")[0].contents[3].contents ])\n",
    "            except Exception:\n",
    "                if not soup.find_all('div', id=\"description\"):\n",
    "                    Plot = ''\n",
    "                else:\n",
    "                    Plot = ' '.join([remove_html_tags(str(c)) for c in soup.find_all('div', id=\"description\")[0].contents[1].contents ])\n",
    "            if Plot:\n",
    "                if detect(Plot) != 'en':\n",
    "                    print('Article removed:', html_file)\n",
    "                    continue\n",
    "            bookTitle = soup.find_all('h1')[0].contents[0].replace('\\n', '').strip()\n",
    "            bookSeries = soup.find_all('h2', id='bookSeries')[0].text.replace('\\n', '').strip()\n",
    "            bookAuthors = ', '.join([soup.find_all('span', itemprop='name')[i].contents[0] for i in range(\n",
    "                len(soup.find_all('span', itemprop='name')))])\n",
    "            ratingValue = soup.find_all('span', itemprop='ratingValue')[0].contents[0].replace('\\n', '').strip()\n",
    "            ratingCount = soup.find_all('meta', itemprop=\"ratingCount\")[0]['content']\n",
    "            reviewCount = soup.find_all('meta', itemprop=\"reviewCount\")[0]['content']\n",
    "            try:\n",
    "                NumberofPages = re.findall(r'\\d+', soup.find_all('span', itemprop=\"numberOfPages\")[0].contents[0])[0]\n",
    "            except:\n",
    "                if not soup.find_all('span', itemprop=\"bookFormat\"):\n",
    "                    NumberofPages = ''\n",
    "                else:\n",
    "                    NumberofPages = soup.find_all('span', itemprop=\"bookFormat\")[0].contents[0]\n",
    "            try:\n",
    "                temp_date = soup.find_all('div', id='details')[0].find_all('div', {\"class\": \"row\"})[1].text.split('\\n')[\n",
    "                    2].split()\n",
    "            except:\n",
    "                try:\n",
    "                    temp_date = soup.find_all('div', id='details')[0].find_all('div', {\"class\": \"row\"})[0].contents[0].split('\\n')[\n",
    "                        2].split()\n",
    "                except:\n",
    "                    try:\n",
    "                        temp_date = soup.find_all('div', id='details')[0].find_all('nobr', {\"class\": \"greyText\"})[0].contents[0].split('\\n')[1].split()[-3:]\n",
    "                    except:\n",
    "                        temp_date = ''\n",
    "            PublishingDate = ' '.join(temp_date)\n",
    "            characters = []\n",
    "            settings = []\n",
    "            for i in range(1, len(soup.find_all('div', id=\"bookDataBox\")[0].find_all('a'))):\n",
    "                if re.match(r'/characters/', soup.find_all('div', id=\"bookDataBox\")[0].find_all('a')[i].attrs['href']):\n",
    "                    characters.append(soup.find_all('div', id=\"bookDataBox\")[0].find_all('a')[i].text)\n",
    "                elif re.match(r'/places/', soup.find_all('div', id=\"bookDataBox\")[0].find_all('a')[i].attrs['href']):\n",
    "                    settings.append(soup.find_all('div', id=\"bookDataBox\")[0].find_all('a')[i].text)\n",
    "            characters = ', '.join(characters)\n",
    "            settings = ', '.join(settings)\n",
    "            url = soup.find_all('link', rel='canonical')[0].attrs['href']\n",
    "\n",
    "            final_list = [bookTitle, bookSeries, bookAuthors, ratingValue, ratingCount, reviewCount,\n",
    "                          Plot, NumberofPages, PublishingDate, characters, settings, url]\n",
    "            \n",
    "            filename = 'data/tsv_files/book' + re.findall(r'\\d+', html_file)[0] + '.tsv'\n",
    "            \n",
    "            write_tsv_file(filename, final_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# None parallel approach\n",
    "for i in range(1,301):\n",
    "    print(i)\n",
    "    try:\n",
    "        parse_html_in_folder('data/page_' + str(i))\n",
    "    except:\n",
    "        print('FOLDER NOT PARSED:', i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel approach\n",
    "if __name__ == '__main__':\n",
    "    with Pool(8) as p:\n",
    "        print(p.map(multi_processing_functions.parse_html_in_folder, \n",
    "                    ['../data_html/' + i for i in os.listdir('../data_html')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0. Pre-process of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(plot):\n",
    "    #This allow us to identify stop word in english\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    word_tokens = word_tokenize(plot)\n",
    "    filtered_sentence = [w.lower() for w in word_tokens if w.lower() not in stopwords and len(w) > 1]\n",
    "\n",
    "    text = ' '.join(filtered_sentence)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(plot): \n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    # TODO: Infinite possibilities….\n",
    "    text = tokenizer.tokenize(plot)\n",
    "    clean_punctuation = ' '.join(text)\n",
    "    return clean_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stemming(sentence):\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    words = word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    \n",
    "    for w in words:\n",
    "        stem_sentence.append(ps.stem(w))\n",
    "\n",
    "    text = \" \".join(stem_sentence)\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_lemma(sentence):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(sentence)\n",
    "    lemma = []\n",
    "    for token in doc:\n",
    "        lemma.append(token.lemma_)\n",
    "    text = ' '.join(lemma)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_pre_process(text):\n",
    "    \"\"\" Function to process everything at once \"\"\"\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stop_words(text)\n",
    "    text = remove_lemma(text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(path, destiny_path='clean_tsv_files', columns_to_process= [0, 1, 2, 6, 8, 9], \n",
    "                   columns_to_include=[3, 4, 5]):\n",
    "    \"\"\" Pre-process data \n",
    "    \n",
    "    0: bookTitle \n",
    "    1: bookSeries \n",
    "    2: bookAuthors \n",
    "    3: ratingValue \n",
    "    4: ratingCount \n",
    "    5: reviewCount\n",
    "    6: Plot\n",
    "    8: PublishingDate \n",
    "    9: characters\n",
    "    10: settings\n",
    "    11: url\n",
    "    \n",
    "    \"\"\"\n",
    "    tsv_files = os.listdir(path)\n",
    "    for filename in tsv_files:\n",
    "        print(filename)\n",
    "        filename = path + filename\n",
    "        book_data = read_tsv(filename)\n",
    "        list_of_processed_fields = columns_to_process\n",
    "        total_list_of_fields = list_of_processed_fields + columns_to_include\n",
    "        total_list_of_fields.sort()\n",
    "        pre_processed_string = []\n",
    "        for i in total_list_of_fields:\n",
    "            if i in columns_to_process:\n",
    "                pre_processed_string.append(global_pre_process(book_data[i]))\n",
    "            else:\n",
    "                pre_processed_string.append(book_data[i])\n",
    "        \n",
    "        # Keep only the year of publication\n",
    "        if 8 in columns_to_process:\n",
    "            try:\n",
    "                pre_processed_string[7] = final = [re.findall(r'\\d+', pre_processed_string[7])[i] for i in \n",
    "                                                   range(len(re.findall(r'\\d+', pre_processed_string[7]))) if \n",
    "                                                   len(re.findall(r'\\d+', pre_processed_string[7])[i]) == 4][0]\n",
    "            except:\n",
    "                pre_processed_string[7] = ''\n",
    "        \n",
    "        filename = 'data/' + destiny_path + '/book' + re.findall(r'\\d+', filename)[0] + '.tsv'\n",
    "            \n",
    "        write_tsv(filename, pre_processed_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book3451.tsv\n",
      "book3749.tsv\n"
     ]
    }
   ],
   "source": [
    "pre_processing('data/single_tsv/', destiny_path='clean_tsv_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_tsv_files('data/clean_tsv_files/', destiny='data/clean_tsv_files/clean_tsv_files.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Create your index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(path, columns):\n",
    "    tsv_files = os.listdir(path)\n",
    "    vocabulary = {}\n",
    "    count = 1\n",
    "    inverted_index = {}\n",
    "    for filename in tsv_files:\n",
    "        d_id = int(re.findall(r'\\d+', filename)[0])\n",
    "        filename = path + filename\n",
    "        text = read_tsv(filename)\n",
    "        if isinstance(columns, list):\n",
    "            text = (' '.join([text[i] for i in columns])).split(' ')\n",
    "        else:\n",
    "            raise('Column must be a list')\n",
    "            \n",
    "        for word in text:\n",
    "            if word not in vocabulary: \n",
    "                vocabulary[word] = count\n",
    "                inverted_index[count] = [d_id]\n",
    "            #    print(inverted_index)\n",
    "                count +=1\n",
    "            else:\n",
    "                key = vocabulary[word]\n",
    "                if d_id not in inverted_index[key]:\n",
    "                     inverted_index[key].append(d_id)\n",
    "    return vocabulary, inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_test, inverted_index_test = get_vocabulary('data/clean_tsv_files_test/', columns=[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json('data/inverted_index_test.json', inverted_index_test)\n",
    "write_json('data/vocabulary_dict_test.json', vocabulary_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pointer_values(pointer, index_list):\n",
    "    \"\"\" Based on a set of pointer values get the documents \"\"\"\n",
    "    values = []\n",
    "    for i in range(len(pointer)):\n",
    "        values.append(index_list[i][pointer[i]])\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pointer(values, pointer):\n",
    "    \"\"\" Given the values, compute the minimum and update the pointer accordingly based on their minimum \"\"\"\n",
    "    mins = np.where(values == np.min(values))[0]\n",
    "    for i in range(0, len(mins)):\n",
    "        pointer[mins[i]] = pointer[mins[i]] + 1 \n",
    "    return pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_function(query, index, vocabulary_integer):\n",
    "    \"\"\" Given a query find the documents in which these appear based on the index \"\"\"\n",
    "    \n",
    "    # Pre-process query \n",
    "    query = global_pre_process(query)\n",
    "    \n",
    "    # Query to list of strings\n",
    "    query_list = query.split()\n",
    "    \n",
    "    # Map strings to integer based on dict\n",
    "    integer_list = [vocabulary_integer[i] for i in query_list]\n",
    "    \n",
    "    # Start to look for the intersection of the query in the index\n",
    "    total_query_documents = [sorted(index[i]) for i in integer_list]\n",
    "    \n",
    "    # Generate a list with the pointer values\n",
    "    pointers = np.full(len(total_query_documents), 0)\n",
    "    values = np.full(len(total_query_documents), 0)\n",
    "    \n",
    "    # List where intersection documents will be stored\n",
    "    intersection = []\n",
    "\n",
    "    # Compute the document in which the search should stop\n",
    "    max_list = np.array([max(total_query_documents[i]) for i in range(len(total_query_documents))])\n",
    "\n",
    "    try:\n",
    "        # Loop over all elements stopping at the minimum between all documents\n",
    "        while np.any(values != max_list):\n",
    "            # Get the documents based on the pointer\n",
    "            values = get_pointer_values(pointer = pointers, \n",
    "                                        index_list = total_query_documents)\n",
    "            # If all values are equal we have found a match and all the pointer values are increased by one\n",
    "            if len(set(values)) == 1:\n",
    "                intersection.append(values[0])\n",
    "                pointers += 1\n",
    "            # If all values are not equal increase the values of the minimum pointers\n",
    "            else:\n",
    "                pointers = update_pointer(values, pointers)\n",
    "    except:\n",
    "        intersection = sorted(list(set.intersection(*map(set,total_query_documents))))\n",
    "    \n",
    "    assert intersection == sorted(list(set.intersection(*map(set,total_query_documents)))), 'Algorithm is not returning same result as python implementation'\n",
    "    \n",
    "    return intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_image_html(path):\n",
    "    return '<img src=\"'+ path + '\" style=max-height:124px;\"/>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(book_ids, tsv_path = 'data/tsv_files'):\n",
    "    output = pd.DataFrame(columns=['BookTitle', 'Plot', 'Url'])\n",
    "    for book_id in book_ids:\n",
    "        tsv_file = tsv_path + '/book' + str(book_id) + '.tsv'\n",
    "        data = read_tsv(tsv_file)\n",
    "        output = output.append(pd.Series([data[0], data[6], data[-1]], index=output.columns), ignore_index=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine_1(query, inverted_index, vocabulary, tsv_path):\n",
    "    query_results = query_function(query, inverted_index, vocabulary)\n",
    "    output = show_results(query_results, tsv_path)\n",
    "    output = HTML(output.to_html(escape=False,\n",
    "                                 formatters=dict(column_name_with_image_links=path_to_image_html)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>Could you survive on your own in the wild, with every one out to make sure you don't live to see the morning?   In the ruins of a place once known as North America lies the nation of Panem, a shining Capitol surrounded by twelve outlying districts. The Capitol is harsh and cruel and keeps the districts in line by forcing them all to send one boy and one girl between the ages of twelve and eighteen to participate in the annual Hunger Games, a fight to the death on live TV.   Sixteen-year-old Katniss Everdeen, who lives alone with her mother and younger sister, regards it as a death sentence when she steps forward to take her sister's place in the Games. But Katniss has been close to dead before—and survival, for her, is second nature. Without really meaning to, she becomes a contender. But if she is to win, she will have to start making choices that weight survival against humanity and life against love.</td>\n",
       "      <td>https://www.goodreads.com/book/show/2767052-the-hunger-games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Iron King</td>\n",
       "      <td>Meghan Chase has a secret destiny; one she could never have imagined.   Something has always felt slightly off in Meghan's life, ever since her father disappeared before her eyes when she was six. She has never quite fit in at school or at home.   When a dark stranger begins watching her from afar, and her prankster best friend becomes strangely protective of her, Meghan senses that everything she's known is about to change.   But she could never have guessed the truth - that she is the daughter of a mythical faery king and is a pawn in a deadly war. Now Meghan will learn just how far she'll go to save someone she cares about, to stop a mysterious evil, no faery creature dare face; and to find love with a young prince who might rather see her dead than let her touch his icy heart.</td>\n",
       "      <td>https://www.goodreads.com/book/show/6644117-the-iron-king</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = search_engine_1('one could', inverted_index_test, vocabulary_test, 'data/test_tsv')\n",
    "search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tfidf(path, vocabulary, inverted_index, json_name='tfidf.json', columns=[6]):\n",
    "    '''Path: clean tsv file \n",
    "    param column: If a list is provided the score will be computed over several columns\n",
    "    '''\n",
    "    tsv_files = os.listdir(path)\n",
    "    no_of_documents = len(tsv_files)\n",
    "    \n",
    "    # number of words in vacabulary\n",
    "    no_of_words_in_vocab = len(vocabulary)\n",
    "    \n",
    "    tfidfDicts = {}\n",
    "    \n",
    "    for filename in tsv_files:\n",
    "        d_id = int(re.findall(r'\\d+', filename)[0])\n",
    "        filename = path + filename\n",
    "        \n",
    "        # read plot from file name\n",
    "        if isinstance(columns, list):\n",
    "            text = read_tsv(filename)\n",
    "            text = (' '.join([text[i] for i in columns])).split(' ')\n",
    "        else:\n",
    "            raise('Column must be a list')\n",
    "            \n",
    "        no_of_words_in_plot = len(text)\n",
    "        # Create a vector\n",
    "        tfDict = dict.fromkeys((i for i in range(1, no_of_words_in_vocab+1)), 0)\n",
    "        \n",
    "        \n",
    "        for word in text:\n",
    "            index = vocabulary[word]\n",
    "            tfDict[index] +=1\n",
    "        \n",
    "        tfidfDict = {}\n",
    "        \n",
    "        for key, value in tfDict.items():\n",
    "            if value != 0:\n",
    "                \n",
    "                no_of_documents_appeared = len(inverted_index[key])\n",
    "\n",
    "                tfidf = (value/no_of_words_in_plot) * np.log(no_of_documents/no_of_documents_appeared)\n",
    "\n",
    "                tfidfDict[key] = float('{:.4f}'.format(tfidf))\n",
    "                \n",
    "                #print([word for word, index in vocabulary.items() if index == key], value, no_of_words_in_plot, no_of_documents, no_of_documents_appeared, tfidfDict[key])\n",
    "        \n",
    "        tfidfDicts[d_id] = tfidfDict\n",
    "        \n",
    "    documents = collections.OrderedDict(sorted(tfidfDicts.items()))\n",
    "    write_json('data/' + json_name, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 54.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorize_tfidf('data/clean_tsv_files_test/', vocabulary_test, inverted_index_test, json_name='tfidf_test.json')\n",
    "\n",
    "tfidfDicts_test = read_json('data/tfidf_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine(doc, query):\n",
    "    intersection = set(doc.keys()) & set(query.keys())\n",
    "    numerator = sum([doc[x] * query[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([doc[x] ** 2 for x in list(doc.keys())])\n",
    "    sum2 = sum([query[x] ** 2 for x in list(query.keys())])\n",
    "    denominator = np.sqrt(sum1) * np.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results_cosine_similarity(book_id, cosine_similarity, tsv_path = 'data/test_tsv'):\n",
    "    tsv_file = tsv_path + '/book' + str(book_id) + '.tsv'\n",
    "    data = read_tsv(tsv_file)\n",
    "    #       Title    Plot     Url\n",
    "    data = [data[0], data[6], data[-1]]\n",
    "    output = (cosine_similarity, data)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results_cosine_similarity_and_ratings(book_id, cosine_similarity, tsv_path = 'data/tsv_files'):\n",
    "    tsv_file = tsv_path + '/book' + str(book_id) + '.tsv'\n",
    "    raw_data = read_tsv(tsv_file)\n",
    "    #       Title        Plot         Url\n",
    "    data = [raw_data[0], raw_data[6], raw_data[-1]]\n",
    "    #                            RatingValue         ratingCount         reviewCount\n",
    "    output = (cosine_similarity, float(raw_data[3]), float(raw_data[4]), float(raw_data[5]), data)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine_2(query, inverted_index, vocabulary, tfidf_scores_dict, tsv_path = 'data/tsv_files', k = 10):\n",
    "   \n",
    "    output = pd.DataFrame(columns=['BookTitle', 'Plot', 'Url', 'Similarity'])\n",
    "    documents_with_query_words = query_function(query, inverted_index, vocabulary)\n",
    "    queryed_documents_tfidf = {key: value for key, value in tfidf_scores_dict.items() if key in documents_with_query_words}\n",
    "    heap_data = []\n",
    "    \n",
    "    # pre-process query\n",
    "    query = global_pre_process(query)\n",
    "    \n",
    "    # vectorize query\n",
    "    vector_query = {}\n",
    "    for word in query.split(' '):\n",
    "        index = vocabulary[word]\n",
    "        vector_query[index] = 1\n",
    "    \n",
    "    for i in queryed_documents_tfidf.keys():\n",
    "        similarity = get_cosine(queryed_documents_tfidf[i], vector_query)\n",
    "        x = show_results_cosine_similarity(i, similarity, tsv_path)\n",
    "        if len(heap_data) < k:\n",
    "            heapq.heappush(heap_data, x)\n",
    "        else:\n",
    "            heapq.heappushpop(heap_data, x)\n",
    "    for i in range(len(heap_data)):\n",
    "        output = output.append(pd.Series([heap_data[-(i+1)][1][0], heap_data[-(i+1)][1][1], \n",
    "                                          heap_data[-(i+1)][1][2], heap_data[-(i+1)][0]], \n",
    "                                         index=output.columns), ignore_index=True) \n",
    "    output = output.sort_values(by='Similarity', ascending=False)\n",
    "    output = HTML(output.to_html(escape=False,\n",
    "                                 formatters=dict(column_name_with_image_links=path_to_image_html)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Iron King</td>\n",
       "      <td>Meghan Chase has a secret destiny; one she could never have imagined.   Something has always felt slightly off in Meghan's life, ever since her father disappeared before her eyes when she was six. She has never quite fit in at school or at home.   When a dark stranger begins watching her from afar, and her prankster best friend becomes strangely protective of her, Meghan senses that everything she's known is about to change.   But she could never have guessed the truth - that she is the daughter of a mythical faery king and is a pawn in a deadly war. Now Meghan will learn just how far she'll go to save someone she cares about, to stop a mysterious evil, no faery creature dare face; and to find love with a young prince who might rather see her dead than let her touch his icy heart.</td>\n",
       "      <td>https://www.goodreads.com/book/show/6644117-the-iron-king</td>\n",
       "      <td>0.127684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>Could you survive on your own in the wild, with every one out to make sure you don't live to see the morning?   In the ruins of a place once known as North America lies the nation of Panem, a shining Capitol surrounded by twelve outlying districts. The Capitol is harsh and cruel and keeps the districts in line by forcing them all to send one boy and one girl between the ages of twelve and eighteen to participate in the annual Hunger Games, a fight to the death on live TV.   Sixteen-year-old Katniss Everdeen, who lives alone with her mother and younger sister, regards it as a death sentence when she steps forward to take her sister's place in the Games. But Katniss has been close to dead before—and survival, for her, is second nature. Without really meaning to, she becomes a contender. But if she is to win, she will have to start making choices that weight survival against humanity and life against love.</td>\n",
       "      <td>https://www.goodreads.com/book/show/2767052-the-hunger-games</td>\n",
       "      <td>0.102831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_engine_2(query = 'could one', inverted_index = inverted_index_test, \n",
    "                vocabulary=vocabulary_test, tfidf_scores_dict=tfidfDicts_test,\n",
    "                tsv_path = 'data/test_tsv', k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a new score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After giving it a lot of thought, and based on the information we have, we have decided to explore two different approaches:\n",
    "1. Weighted average of the cosine_similarity, ratingValue, ratingCount and reviewCount. This is not the ideal scenario since weights are not justifiable without any data on the users search history.\n",
    "2. Normalize the ratingValue, ratingCount and reviewCount and multiply their sum against the cosine similarity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 90.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocabulary_test_question3, inverted_index_test_question3 = get_vocabulary('data/clean_tsv_files_test/',\n",
    "                columns=[0, 1, 2, 6, 7, 8])\n",
    "\n",
    "write_json('data/inverted_index_test_question3.json', inverted_index_test_question3)\n",
    "write_json('data/vocabulary_dict_test_question3.json', vocabulary_test_question3)\n",
    "\n",
    "vectorize_tfidf('data/clean_tsv_files_test/', vocabulary_test_question3, \n",
    "                inverted_index_test_question3, json_name='tfidf_test_question3.json',\n",
    "                columns=[0, 1, 2, 6, 7, 8])\n",
    "\n",
    "tfidfDicts_test_question3 = read_json('data/tfidf_test_question3.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine_3(query, inverted_index, vocabulary, tfidf_scores_dict, tsv_path = 'data/tsv_files', k = 10,\n",
    "                    new_score='cosine_normalizer', weights=None):\n",
    "    \"\"\"\n",
    "    There are currently two alterantives for the computation of the new score:\n",
    "    - cosine_normalizer: Normalize all quantitative values and multiply against cosine similiarity\n",
    "    - weighted_average: Give weights to all features based on expert judgement (provide weights as list required!!)\n",
    "    - weights: list with weights:\n",
    "        weights[0]: weight for cosine_similiarity\n",
    "        weights[1]: weight for ratingValue\n",
    "        weights[2]: weight for ratingCount\n",
    "        weights[3]: weight for reviewCount\n",
    "\n",
    "    \"\"\"\n",
    "   \n",
    "    output = pd.DataFrame(columns=['BookTitle', 'Plot', 'Url', 'Score'])\n",
    "    documents_with_query_words = query_function(query, inverted_index, vocabulary)\n",
    "    queryed_documents_tfidf = {key: value for key, value in tfidf_scores_dict.items() if key in documents_with_query_words}\n",
    "    heap_data = []\n",
    "    \n",
    "    # pre-process query\n",
    "    query = global_pre_process(query)\n",
    "    \n",
    "    # vectorize query\n",
    "    vector_query = {}\n",
    "    for word in query.split(' '):\n",
    "        index = vocabulary[word]\n",
    "        vector_query[index] = 1\n",
    "        \n",
    "    # Get max_ratingCount and max_reviewCount\n",
    "    if new_score == 'cosine_normalizer':\n",
    "        ratingValue_list = []\n",
    "        ratingCount_list = []\n",
    "        reviewCount_list = []\n",
    "        for i in queryed_documents_tfidf.keys():\n",
    "            rating_review = read_tsv(tsv_path + '/book' + str(i) + '.tsv')\n",
    "            ratingValue_list.append(float(rating_review[3]))\n",
    "            ratingCount_list.append(int(rating_review[4]))\n",
    "            reviewCount_list.append(int(rating_review[5]))\n",
    "\n",
    "        max_ratingValue = max(ratingValue_list)\n",
    "        max_ratingCount = max(ratingCount_list)\n",
    "        max_reviewCount = max(reviewCount_list)\n",
    "    \n",
    "    # Compute cosine over all intersected documents\n",
    "    for i in queryed_documents_tfidf.keys():\n",
    "        similarity = get_cosine(queryed_documents_tfidf[i], vector_query)\n",
    "        temp = show_results_cosine_similarity_and_ratings(i, similarity, tsv_path)\n",
    "        if new_score == 'weighted_average':\n",
    "            score = temp[0]*weights[0] + temp[1]*weights[1] + temp[2]*weights[2] + temp[3]*weights[3]\n",
    "            x = (score, temp[4])\n",
    "        elif new_score == 'cosine_normalizer':\n",
    "            score = temp[0]*(temp[1]/max_ratingValue + temp[2]/max_ratingCount + temp[3]/max_reviewCount)\n",
    "            x = (score, temp[4])\n",
    "        else:\n",
    "            raise('New score method is not implemented')\n",
    "            \n",
    "        if len(heap_data) < k:\n",
    "            heapq.heappush(heap_data, x)\n",
    "        else:\n",
    "            heapq.heappushpop(heap_data, x)\n",
    "    \n",
    "        \n",
    "    for i in range(len(heap_data)):\n",
    "        output = output.append(pd.Series([heap_data[-(i+1)][1][0], heap_data[-(i+1)][1][1], \n",
    "                                          heap_data[-(i+1)][1][2], heap_data[-(i+1)][0]], \n",
    "                                         index=output.columns), ignore_index=True) \n",
    "    output = output.sort_values(by='Score', ascending=False)\n",
    "    output = HTML(output.to_html(escape=False,\n",
    "                                 formatters=dict(column_name_with_image_links=path_to_image_html)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>Could you survive on your own in the wild, with every one out to make sure you don't live to see the morning?   In the ruins of a place once known as North America lies the nation of Panem, a shining Capitol surrounded by twelve outlying districts. The Capitol is harsh and cruel and keeps the districts in line by forcing them all to send one boy and one girl between the ages of twelve and eighteen to participate in the annual Hunger Games, a fight to the death on live TV.   Sixteen-year-old Katniss Everdeen, who lives alone with her mother and younger sister, regards it as a death sentence when she steps forward to take her sister's place in the Games. But Katniss has been close to dead before—and survival, for her, is second nature. Without really meaning to, she becomes a contender. But if she is to win, she will have to start making choices that weight survival against humanity and life against love.</td>\n",
       "      <td>https://www.goodreads.com/book/show/2767052-the-hunger-games</td>\n",
       "      <td>0.183938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Iron King</td>\n",
       "      <td>Meghan Chase has a secret destiny; one she could never have imagined.   Something has always felt slightly off in Meghan's life, ever since her father disappeared before her eyes when she was six. She has never quite fit in at school or at home.   When a dark stranger begins watching her from afar, and her prankster best friend becomes strangely protective of her, Meghan senses that everything she's known is about to change.   But she could never have guessed the truth - that she is the daughter of a mythical faery king and is a pawn in a deadly war. Now Meghan will learn just how far she'll go to save someone she cares about, to stop a mysterious evil, no faery creature dare face; and to find love with a young prince who might rather see her dead than let her touch his icy heart.</td>\n",
       "      <td>https://www.goodreads.com/book/show/6644117-the-iron-king</td>\n",
       "      <td>0.100873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_engine_3(query = 'could one', inverted_index = inverted_index_test_question3, \n",
    "                vocabulary=vocabulary_test_question3, tfidf_scores_dict=tfidfDicts_test_question3,\n",
    "                tsv_path = 'data/test_tsv', k = 10,\n",
    "                new_score='cosine_normalizer', weights=[0.5, 0.2, 0.1, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Iron King</td>\n",
       "      <td>Meghan Chase has a secret destiny; one she could never have imagined.   Something has always felt slightly off in Meghan's life, ever since her father disappeared before her eyes when she was six. She has never quite fit in at school or at home.   When a dark stranger begins watching her from afar, and her prankster best friend becomes strangely protective of her, Meghan senses that everything she's known is about to change.   But she could never have guessed the truth - that she is the daughter of a mythical faery king and is a pawn in a deadly war. Now Meghan will learn just how far she'll go to save someone she cares about, to stop a mysterious evil, no faery creature dare face; and to find love with a young prince who might rather see her dead than let her touch his icy heart.</td>\n",
       "      <td>https://www.goodreads.com/book/show/6644117-the-iron-king</td>\n",
       "      <td>0.100895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>Could you survive on your own in the wild, with every one out to make sure you don't live to see the morning?   In the ruins of a place once known as North America lies the nation of Panem, a shining Capitol surrounded by twelve outlying districts. The Capitol is harsh and cruel and keeps the districts in line by forcing them all to send one boy and one girl between the ages of twelve and eighteen to participate in the annual Hunger Games, a fight to the death on live TV.   Sixteen-year-old Katniss Everdeen, who lives alone with her mother and younger sister, regards it as a death sentence when she steps forward to take her sister's place in the Games. But Katniss has been close to dead before—and survival, for her, is second nature. Without really meaning to, she becomes a contender. But if she is to win, she will have to start making choices that weight survival against humanity and life against love.</td>\n",
       "      <td>https://www.goodreads.com/book/show/2767052-the-hunger-games</td>\n",
       "      <td>0.061313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_engine_2(query = 'could one', inverted_index = inverted_index_test_question3, \n",
    "                vocabulary=vocabulary_test_question3, tfidf_scores_dict=tfidfDicts_test_question3,\n",
    "                tsv_path = 'data/test_tsv', k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make a nice visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_series(path, num_series=20, series_to_include=['Harry Potter']):\n",
    "    \"\"\" \n",
    "    \n",
    "    \"\"\"\n",
    "    bookSeries = {}\n",
    "    tsv_files = os.listdir(path)\n",
    "    i = 0\n",
    "    for filename in tsv_files:\n",
    "        i += 1\n",
    "        if i%1000 == 0:\n",
    "            print(i)\n",
    "        filename = path + filename\n",
    "        book_data = read_tsv(filename)\n",
    "        clean_series = re.sub(r'[^a-zA-Z0-9]', ' ', book_data[1]).split()\n",
    "        series_name = re.sub(r'[^a-zA-Z]', ' ', book_data[1]).rstrip().lstrip()\n",
    "        # If the book is part of a series and the series is one single book\n",
    "        if (series_name != '') & (len([i for i in clean_series if bool(re.match(r'\\d+', i))]) == 1):\n",
    "            if series_name not in bookSeries:\n",
    "                # Make sure we only take the first 20 series\n",
    "                if (len(bookSeries.keys()) < num_series) | (series_name in series_to_include):\n",
    "                    split_date = re.findall(r'\\d+', book_data[8])\n",
    "                    year = [i for i in split_date if len(i) == 4][0]\n",
    "                    bookSeries[series_name] = [[' '.join(clean_series), year, book_data[7], book_data[-1], filename]]\n",
    "            else:                \n",
    "                split_date = re.findall(r'\\d+', book_data[8])\n",
    "                try:\n",
    "                    year = [i for i in split_date if len(i) == 4][0]\n",
    "                except:\n",
    "                    year = book_data[8]\n",
    "                bookSeries[series_name].append([' '.join(clean_series), year, book_data[7], book_data[-1], filename])\n",
    "\n",
    "    return bookSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "book_series = get_book_series('data/tsv_files/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: It would be cool if we can put this into a dropdown of some kind\n",
    "def plot_series(series_name, bookSeries_dict = book_series):\n",
    "    publish_years = [i[1] for i in bookSeries_dict[series_name]]\n",
    "    pages = [i[2] for i in bookSeries_dict[series_name]]\n",
    "    df = pd.DataFrame(columns=['Year', 'Pages'])\n",
    "    df['Year'] = publish_years\n",
    "    df['Pages'] = pages\n",
    "    print(df)\n",
    "    df = df.sort_values(by='Year', ascending=True)\n",
    "    df['Years Since Publishment'] = df.Year - df.Year.min()\n",
    "    df['Pages of Book Series'] = df.Pages.cumsum()\n",
    "    df.plot(x = 'Years Since Publishment', y = 'Pages of Book Series', title = series_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series('Harry Potter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, Dropdown\n",
    "\n",
    "dropdown_bookSeries = Dropdown(options = list(book_series.keys()))\n",
    "    \n",
    "@interact(series_name = dropdown_bookSeries)\n",
    "def dropdown_series(series_name):\n",
    "    plot_series(series_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DELETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "[bookTitle, bookSeries, bookAuthors, ratingValue, ratingCount, reviewCount,\n",
    "                          Plot, NumberofPages, PublishingDate, characters, settings, url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beware the Jabberwock, my son! \\n The jaws that bite, the claws that catch! \\n Beware the Jubjub bird, and shun \\n The frumious Bandersnatch!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/test_tsv/'\n",
    "tsv_files = os.listdir(path)\n",
    "\n",
    "list_tsv = []\n",
    "pd.DataFrame(columns=['bookid', 'bookTitle', 'bookSeries', 'bookAuthors', 'ratingValue', \n",
    "                               'ratingCount', 'reviewCount', 'Plot', 'NumberofPages', 'PublishingDate', \n",
    "                               'characters', 'settings', 'url'])\n",
    "\n",
    "def merge_tsv_files(path, destiny='data/tsv_files/tsv_files.tsv')\n",
    "    tsv_files = os.listdir(path)\n",
    "    list_tsv = []\n",
    "    for filename in tsv_files:\n",
    "        book_id = re.findall(r'\\d+', filename)[0]\n",
    "        filename = path + filename\n",
    "        book_data = read_tsv(filename)\n",
    "        book_data = [book_id] + book_data\n",
    "        list_tsv.append(book_data)\n",
    "    df=pd.DataFrame(list_tsv,columns=['bookid', 'bookTitle', 'bookSeries', 'bookAuthors', 'ratingValue', \n",
    "                                     'ratingCount', 'reviewCount', 'Plot', 'NumberofPages', 'PublishingDate', \n",
    "                                     'characters', 'settings', 'url'])\n",
    "    df.to_csv('data/tsv_files/tsv_files.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1',\n",
       "  'The Hunger Games',\n",
       "  '(The Hunger Games #1)',\n",
       "  'Suzanne Collins',\n",
       "  '4.33',\n",
       "  '6409198',\n",
       "  '172562',\n",
       "  \"Could you survive on your own in the wild, with every one out to make sure you don't live to see the morning?   In the ruins of a place once known as North America lies the nation of Panem, a shining Capitol surrounded by twelve outlying districts. The Capitol is harsh and cruel and keeps the districts in line by forcing them all to send one boy and one girl between the ages of twelve and eighteen to participate in the annual Hunger Games, a fight to the death on live TV.   Sixteen-year-old Katniss Everdeen, who lives alone with her mother and younger sister, regards it as a death sentence when she steps forward to take her sister's place in the Games. But Katniss has been close to dead before—and survival, for her, is second nature. Without really meaning to, she becomes a contender. But if she is to win, she will have to start making choices that weight survival against humanity and life against love.\",\n",
       "  '374',\n",
       "  'September 14th 2008',\n",
       "  'Katniss Everdeen, Peeta Mellark, Cato (Hunger Games), Primrose Everdeen, Gale Hawthorne, Effie Trinket, Haymitch Abernathy, Cinna, President Coriolanus Snow, Rue, Flavius, Lavinia (Hunger Games), Marvel, Glimmer, Clove, Foxface, Thresh, Greasy Sae, Madge Undersee, Caesar Flickerman, Claudius Templesmith, Octavia (Hunger Games), Portia (hunger Games)',\n",
       "  'District 12, Panem, Capitol, Panem, Panem',\n",
       "  'https://www.goodreads.com/book/show/2767052-the-hunger-games'],\n",
       " ['2',\n",
       "  'Harry Potter and the Order of the Phoenix',\n",
       "  '(Harry Potter #5)',\n",
       "  'J.K. Rowling, Mary GrandPré',\n",
       "  '4.50',\n",
       "  '2525442',\n",
       "  '42741',\n",
       "  'There is a door at the end of a silent corridor. And it’s haunting Harry Pottter’s dreams. Why else would he be waking in the middle of the night, screaming in terror?   Harry has a lot on his mind for this, his fifth year at Hogwarts: a Defense Against the Dark Arts teacher with a personality like poisoned honey; a big surprise on the Gryffindor Quidditch team; and the looming terror of the Ordinary Wizarding Level exams. But all these things pale next to the growing threat of He-Who-Must-Not-Be-Named - a threat that neither the magical government nor the authorities at Hogwarts can stop.   As the grasp of darkness tightens, Harry must discover the true depth and strength of his friends, the importance of boundless loyalty, and the shocking price of unbearable sacrifice.   His fate depends on them all.',\n",
       "  '870',\n",
       "  'September 2004',\n",
       "  'Sirius Black, Draco Malfoy, Ron Weasley, Petunia Dursley, Vernon Dursley, Dudley Dursley, Severus Snape, Rubeus Hagrid, Lord Voldemort, Minerva McGonagall, Neville Longbottom, Fred Weasley, George Weasley, Percy Weasley, Ginny Weasley, Colin Creevey, Filius Flitwick, Gilderoy Lockhart, Lucius Malfoy, Pomona Sprout, Arthur Weasley, Molly Weasley, Cho Chang, Cornelius Fudge, Remus Lupin, Sybil Trelawney, Stan Shunpike, Bellatrix Lestrange, Alastor Moody, Rita Skeeter, Luna Lovegood, Nymphadora Tonks, Dolores Umbridge, Dobby, Kingsley Shacklebolt, Padma Patil, Parvati Patil, Kreacher, Dean Thomas, Seamus Finnigan, Albus Dumbledore, Harry Potter, Hermione Granger, Lavender Brown',\n",
       "  'Hogwarts School of Witchcraft and Wizardry, London, England',\n",
       "  'https://www.goodreads.com/book/show/2.Harry_Potter_and_the_Order_of_the_Phoenix'],\n",
       " ['20',\n",
       "  'Les Misérables',\n",
       "  '',\n",
       "  'Victor Hugo, Lee Fahnestock, Norman MacAfee',\n",
       "  '4.18',\n",
       "  '685616',\n",
       "  '17615',\n",
       "  \"Victor Hugo's tale of injustice, heroism and love follows the fortunes of Jean Valjean, an escaped convict determined to put his criminal past behind him. But his attempts to become a respected member of the community are constantly put under threat: by his own conscience, when, owing to a case of mistaken identity, another man is arrested in his place; and by the relentless investigations of the dogged Inspector Javert. It is not simply for himself that Valjean must stay free, however, for he has sworn to protect the baby daughter of Fantine, driven to prostitution by poverty.\",\n",
       "  '1463',\n",
       "  'March 3rd 1987',\n",
       "  'Javert, Cosette, Fantine, Bishop Myriel, M. & Mme. Thénardier, Marius Pontmercy, Enjolras, Éponine, Gavroche, Azelma, Champmathieu, Fauchelevent, Grantaire, Mademoiselle Gillenorman, Felix Tholomyès, Toussaint, Combeferre, Jean Valjean, Javert, Fantine, Cosette, Marius Pontmercy, Jean Valjean, Police Inspector Javert, Cosette, Fantine, Marius Pontmercy, Éponine, Enjolras, Gavroche, Bishop of Digne, Grantaire, Bahorel, Bossuet',\n",
       "  'Paris',\n",
       "  'https://www.goodreads.com/book/show/24280.Les_Mis_rables'],\n",
       " ['21',\n",
       "  'Fahrenheit 451',\n",
       "  '',\n",
       "  'Ray Bradbury',\n",
       "  '3.99',\n",
       "  '1691745',\n",
       "  '48406',\n",
       "  \"Guy Montag is a fireman. In his world, where television rules and literature is on the brink of extinction, firemen start fires rather than put them out. His job is to destroy the most illegal of commodities, the printed book, along with the houses in which they are hidden.   Montag never questions the destruction and ruin his actions produce, returning each day to his bland life and wife, Mildred, who spends all day with her television 'family'. But then he meets an eccentric young neighbor, Clarisse, who introduces him to a past where people did not live in fear and to a present where one sees the world through the ideas in books instead of the mindless chatter of television.   When Mildred attempts suicide and Clarisse suddenly disappears, Montag begins to question everything he has ever known.\",\n",
       "  '194',\n",
       "  'November 29th 2011',\n",
       "  'Guy Montag, Norman Corwin, Clarisse McClellan, Mildred Montag, Captain Beatty, Professor Faber',\n",
       "  '',\n",
       "  'https://www.goodreads.com/book/show/13079982-fahrenheit-451'],\n",
       " ['362',\n",
       "  'White Fang',\n",
       "  '',\n",
       "  'Jack London, مها محمود صالح',\n",
       "  '3.98',\n",
       "  '154812',\n",
       "  '4706',\n",
       "  \"White Fang is part dog and part wolf, and the lone survivor of his family. In his lonely world, he soon learns to follow the harsh law of the North--kill or be killed. But nothing in White Fang's life can prepare him for the cruel owner who turns him into a vicious killer. Will White Fang ever know the kindness of a gentle master?\",\n",
       "  '252',\n",
       "  'January 1st 2001',\n",
       "  '',\n",
       "  'Canada, Yukon (aka Yukon Territory)',\n",
       "  'https://www.goodreads.com/book/show/43035.White_Fang'],\n",
       " ['363',\n",
       "  'The Final Empire',\n",
       "  '(Mistborn #1)',\n",
       "  'Brandon Sanderson',\n",
       "  '4.44',\n",
       "  '406823',\n",
       "  '28232',\n",
       "  'For a thousand years the ash fell and no flowers bloomed. For a thousand years the Skaa slaved in misery and lived in fear. For a thousand years the Lord Ruler, the \"Sliver of Infinity,\" reigned with absolute power and ultimate terror, divinely invincible. Then, when hope was so long lost that not even its memory remained, a terribly scarred, heart-broken half-Skaa rediscovered it in the depths of the Lord Ruler\\'s most hellish prison. Kelsier \"snapped\" and found in himself the powers of a Mistborn. A brilliant thief and natural leader, he turned his talents to the ultimate caper, with the Lord Ruler himself as the mark.   Kelsier recruited the underworld\\'s elite, the smartest and most trustworthy allomancers, each of whom shares one of his many powers, and all of whom relish a high-stakes challenge. Only then does he reveal his ultimate dream, not just the greatest heist in history, but the downfall of the divine despot.   But even with the best criminal crew ever assembled, Kel\\'s plan looks more like the ultimate long shot, until luck brings a ragged girl named Vin into his life. Like him, she\\'s a half-Skaa orphan, but she\\'s lived a much harsher life. Vin has learned to expect betrayal from everyone she meets, and gotten it. She will have to learn to trust, if Kel is to help her master powers of which she never dreamed.   This saga dares to ask a simple question: What if the hero of prophecy fails?',\n",
       "  '544',\n",
       "  'July 25th 2006',\n",
       "  'Marsh, Vin, Elend Venture, Sazed, Kelsier, Dockson, Hammond, Breeze, Clubs, Spook, The Lord Ruler, Lord Renoux, Yeden, OreSeur',\n",
       "  'Luthadel',\n",
       "  'https://www.goodreads.com/book/show/68428.The_Final_Empire'],\n",
       " ['499',\n",
       "  'The 5th Wave',\n",
       "  '(The 5th Wave #1)',\n",
       "  'Rick Yancey',\n",
       "  '4.04',\n",
       "  '391279',\n",
       "  '29901',\n",
       "  \"After the 1st wave, only darkness remains. After the 2nd, only the lucky escape. And after the 3rd, only the  un lucky survive. After the 4th wave, only one rule applies: trust no one.   Now, it's the dawn of the 5th wave, and on a lonely stretch of highway, Cassie runs from Them. The beings who only look human, who roam the countryside killing anyone they see. Who have scattered Earth's last survivors. To stay alone is to stay alive, Cassie believes, until she meets Evan Walker. Beguiling and mysterious, Evan Walker may be Cassie's only hope for rescuing her brother-or even saving herself. But Cassie must choose: between trust and despair, between defiance and surrender, between life and death. To give up or to get up.\",\n",
       "  '457',\n",
       "  'May 7th 2013',\n",
       "  'Evan Walker, Cassie Sullivan, Sammy Sullivan, Ben Parish, Colonel Alexander',\n",
       "  'Ohio',\n",
       "  'https://www.goodreads.com/book/show/16101128-the-5th-wave'],\n",
       " ['500',\n",
       "  'The Iron King',\n",
       "  '(The Iron Fey #1)',\n",
       "  'Julie Kagawa',\n",
       "  '3.90',\n",
       "  '194743',\n",
       "  '11856',\n",
       "  \"Meghan Chase has a secret destiny; one she could never have imagined.   Something has always felt slightly off in Meghan's life, ever since her father disappeared before her eyes when she was six. She has never quite fit in at school or at home.   When a dark stranger begins watching her from afar, and her prankster best friend becomes strangely protective of her, Meghan senses that everything she's known is about to change.   But she could never have guessed the truth - that she is the daughter of a mythical faery king and is a pawn in a deadly war. Now Meghan will learn just how far she'll go to save someone she cares about, to stop a mysterious evil, no faery creature dare face; and to find love with a young prince who might rather see her dead than let her touch his icy heart.\",\n",
       "  '363',\n",
       "  'January 19th 2010',\n",
       "  \"Puck, Meghan Chase, Grimalkin, Ash - Ashallayn' Darkmyr Tallyn, Ethan Chase, Machina The Iron King\",\n",
       "  'New Orleans, Louisiana, Nevernever',\n",
       "  'https://www.goodreads.com/book/show/6644117-the-iron-king']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(list_tsv,columns=['bookid', 'bookTitle', 'bookSeries', 'bookAuthors', 'ratingValue', \n",
    "                                     'ratingCount', 'reviewCount', 'Plot', 'NumberofPages', 'PublishingDate', \n",
    "                                     'characters', 'settings', 'url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/tsv_files/tsv_files.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Tablet', 250], ['iPhone', 800], ['Laptop', 1200], ['Monitor', 300]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "products = {'Product': ['Tablet','iPhone','Laptop','Monitor'],\n",
    "            'Price': [250,800,1200,300]\n",
    "            }\n",
    "\n",
    "df = pd.DataFrame(products, columns= ['Product', 'Price'])\n",
    "\n",
    "products_list = df.values.tolist()\n",
    "print (products_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
